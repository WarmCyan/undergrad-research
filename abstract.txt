(250 max)

In this paper, we explore two extensions to the architecture developed in the paper FeUdal Networks for Hierarchical Reinforcement Leanring, and how they alter the performance. 1. The original architecture utilized two layers of networks: a manager network and a worker network. We explore how training is affected by utilizing more than two layers. 2. We explore how much modularity the architecture allows by switching manager and worker layers on different gamees, and the feasibility of "pre-trained" worker networks for manager networks to use. (Transfer learning)



In a publication by DeepMind, FeUdal Networks for Hierarchical Reinforcement Learning, a technique known as FeUdal Reinforcement Learning, involving running multiple Reinforcement Learning algorithms at different levels of spatial or temporal resolution, was extended by applying deep learning with the same principles. The two different layers of networks in the architecture operated at different time scales, allowing a higher level manager network to run at a more abstract "overarching" level. Feudal networks achieve significantly higher scores than other deep reinforcement learning architectures on Atari games that are traditionally difficult, such as Montezuma's Revenge. In this paper, we explore two extensions to this architecture and determine how they affect performance. The first extension we look at is utilizing more than two layers of networks. The second is to experiment with how the modularity of this design can be exploited. We look at whether pretrained low-level networks can be plugged into a manager network to improve training time, and whether manager networks can be switched between environments to allow for transfer learning.
